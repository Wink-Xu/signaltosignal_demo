{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Deeplearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (10, 12.0)\n",
    "\n",
    "NUM_HADAMARD = 256\n",
    "\n",
    "sort_index = pd.read_csv('F:/Code/signal-signal_fromscratch/data/collect_sorted_index.csv')\n",
    "sort_index = np.array(sort_index)\n",
    "sort_index = sort_index[:,1:]\n",
    "sort_index = [x-1 for x in sort_index]\n",
    "\n",
    "RD_dir = u'C:/Users/zh/Desktop/hadamard/32RD.png'\n",
    "RD=Image.open(RD_dir)\n",
    "RD = np.array(RD)\n",
    "CC = np.ones((1024,1024))\n",
    "RD_p = 2*RD - CC\n",
    "RD_anti = -1 * RD_p\n",
    "\n",
    "def load_data(data_dir):\n",
    "    data = np.array(pd.read_csv(data_dir))\n",
    "    return data[:,1:]\n",
    "\n",
    "def save_data(x,data_dir):\n",
    "    save_data = pd.DataFrame(x)\n",
    "    save_data.to_csv(data_dir)\n",
    "    \n",
    "    \n",
    "def save_model(sess,ckpt_dir,global_step,model_name='signaltosignal'):\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint_dir = ckpt_dir\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    print(\"Saving model....\")\n",
    "    saver.save(sess,os.path.join(checkpoint_dir,model_name),global_step = global_step)\n",
    "\n",
    "def load_model(ckpt_dir):\n",
    "    print(\"Loading model...\")\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        full_path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "        global_step = int(full_path.split('/')[-1].split('-')[-1])\n",
    "        saver.restore(sess,full_path)\n",
    "        return True,global_step\n",
    "    else :\n",
    "        return False,0\n",
    "\n",
    "\n",
    "    \n",
    "def save_data(x,data_dir):\n",
    "    save_data = pd.DataFrame(x)\n",
    "    save_data.to_csv(data_dir)    \n",
    "\n",
    "def save_image(file_name,original_image,generate_image,noise_image):\n",
    "    original_image = normalize(np.squeeze(original_image))\n",
    "    generate_image = normalize(np.squeeze(generate_image))\n",
    "    noise_image = normalize(np.squeeze(noise_image))\n",
    "    \n",
    "    image = np.concatenate([original_image,generate_image,noise_image],axis = 1)\n",
    "    im = Image.fromarray(image.astype('uint8')).convert('L')\n",
    "    im.save(file_name, 'png')\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))*255   \n",
    "    \n",
    "def inference(input_data):\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "    #    X = tf.placeholder(tf.float32,[1 256],name = 'X1')\n",
    "        weights = tf.get_variable('w1',\n",
    "                              shape = [256,1024],\n",
    "                              dtype = tf.float32,\n",
    "                              initializer = tf.truncated_normal_initializer(stddev=0.03,dtype=tf.float32))\n",
    "        bias = tf.get_variable('b1',shape=[1024],dtype = tf.float32 ,initializer = tf.constant_initializer(0.1))\n",
    "        Y = tf.add(tf.matmul(input_data,weights),bias)\n",
    "        output1 = tf.nn.relu(Y,name = scope.name)\n",
    "    \n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        weights = tf.get_variable(name = 'w2',\n",
    "                                  shape = [1024,1024],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.03,dtype = tf.float32))\n",
    "        bias = tf.get_variable(name ='b2',\n",
    "                              shape = [1024],\n",
    "                              dtype = tf.float32,\n",
    "                              initializer = tf.constant_initializer(0.1))\n",
    "        output2 = tf.nn.relu(tf.add(tf.matmul(output1,weights),bias),name = scope.name)\n",
    "        \n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        weights = tf.get_variable(name = 'w3',\n",
    "                                  shape = [1024,256],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.03,dtype = tf.float32))\n",
    "        bias = tf.get_variable(name ='b3',\n",
    "                              shape = [256],\n",
    "                              dtype = tf.float32,\n",
    "                              initializer = tf.constant_initializer(0.1))\n",
    "        tf.summary.histogram('bias',bias)\n",
    "        output2 = tf.add(tf.matmul(output1,weights),bias,name = scope.name)\n",
    "  \n",
    "    return output2\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(Xdata , Ydata ,Xevaldata,Yevaldata,sort_index,RD_p, learning_rate = 0.0001, epoch = 20 , batch_size = 32):\n",
    "    \n",
    "#    tf.reset_default_graph()\n",
    "    logs = r\"F:/Code/signal-signal_fromscratch/logs/\"\n",
    "    ckpt = r\"F:/Code/signal-signal_fromscratch/checkpoints/\"\n",
    "    sample_dir = r'F:/Code/signal-signal_fromscratch/data/evaluate'\n",
    "    X = tf.placeholder(dtype = tf.float32, shape =[None,256] ,name = 'X')\n",
    "    Y = tf.placeholder(name = 'Y', shape =[None,256] ,dtype = tf.float32)\n",
    "    \n",
    "    output = inference(X)\n",
    "    \n",
    "    losses = tf.reduce_mean(tf.square(output-Y))\n",
    "    \n",
    "    tf.summary.scalar('loss',losses)\n",
    "\n",
    "\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_op = optimizer.minimize(losses) \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    print(\"Start training ...\")\n",
    "    \n",
    "    print(\"Please wait for a while ...\")\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        iter_num = 0\n",
    "        merged = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter(logs,sess.graph)\n",
    "        \n",
    "        numBatch = np.int(Xdata.shape[0]/batch_size)\n",
    "        for i in range(epoch):\n",
    "            for batch_id in range(numBatch):\n",
    "                batch_Xdata = Xdata[batch_id*batch_size:(batch_id+1)*batch_size,:]\n",
    "                batch_Ydata = Ydata[batch_id*batch_size:(batch_id+1)*batch_size,:]\n",
    "                _ , loss,summary = sess.run([train_op , losses,merged] , feed_dict ={X:batch_Xdata ,Y:batch_Ydata })\n",
    "                iter_num += 1\n",
    "                summary_writer.add_summary(summary,iter_num)\n",
    "            if i%2 == 0:\n",
    "                print(\"epoch %d loss %.4f \" % (i,loss))\n",
    "            if i%10 == 0:\n",
    "                print(\"Evaluating ...\")\n",
    "      \n",
    "                for j in range(Xevaldata.shape[0]):\n",
    "                    data = [Xevaldata[j][:]]\n",
    "                    eval_out = sess.run(output,feed_dict = {X:data})\n",
    "                    \n",
    "                    sum1 = np.zeros((32,32))\n",
    "                    sum2 = np.zeros((32,32))\n",
    "                    sum3 = np.zeros((32,32))\n",
    "                    y = 0\n",
    "                    for k in np.squeeze(eval_out):\n",
    "                        y = y+1\n",
    "                        pattern = RD_p[sort_index[y-1]].reshape((1024,1)) \n",
    "                        sum1 = sum1 + Xevaldata[j][y-1] * pattern.reshape((32,32))\n",
    "                        sum2 = sum2 + Yevaldata[j][y-1] * pattern.reshape((32,32))\n",
    "                        sum3 = sum3 + k * pattern.reshape((32,32))\n",
    "                    sum1[0,0] = np.mean(sum1)  \n",
    "                    sum2[0,0] = np.mean(sum2)  \n",
    "                    sum3[0,0] = np.mean(sum3)  \n",
    "          \n",
    "                    save_image(os.path.join(sample_dir, 'test%d_%d.png' % (y, iter_num)),sum2,sum1,sum3)\n",
    "            \n",
    "                save_model(sess,ckpt,iter_num)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train_data_X = np.random.randn(40000,256) * 100\n",
    "train_data_Y = train_data_X + np.random.randn(40000,256)*2\n",
    "\n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "train(train_data_X ,train_data_Y)\n",
    "\n",
    "print(\"The train has been done . -_-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#weights = tf.get_variable('w1',[2,1],initializer=tf.truncated_normal_initializer(stddev=0.03,dtype=tf.float32) )\n",
    "xx = np.random.randn(1,256).astype(np.float32)\n",
    "Y = inference(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "p = sess.run(Y)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable([1,3],'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a= 1\n",
    "b= 1\n",
    "assert a==b,' a!=b '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (10, 12.0)\n",
    "\n",
    "NUM_HADAMARD = 256\n",
    "\n",
    "def normalize(y):\n",
    "    a,b=np.shape(y)\n",
    "    n = np.zeros((a,b))\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            n[i,j] = np.int(y[i,j])\n",
    "    return n\n",
    "\n",
    "\n",
    "RD_dir = u'C:/Users/zh/Desktop/hadamard/32RD.png'\n",
    "RD=Image.open(RD_dir)\n",
    "RD = np.array(RD)\n",
    "CC = np.ones((1024,1024))\n",
    "RD_p = 2*RD - CC\n",
    "RD_anti = -1 * RD_p\n",
    "#RD_n = -1 * RD_p\n",
    "# K = 256\n",
    "# RD_use =np.vstack((RD_p[0:K,:],RD_n[0:K,:]))  #创造需要使用的 Hadamard 矩阵\n",
    "#NUM_PICTURE = 42000\n",
    "#RATIO = 0.1\n",
    "DIM = 1024 \n",
    "i = np.random.randint(100)\n",
    "\n",
    "\n",
    "#rand_pattern = np.random.randn(DIM,DIM)\n",
    "signals = []\n",
    "for i in range(200):\n",
    "    filename1 = 'F:\\Code\\kaggle\\Digit Recognizer\\image\\image' + '%d' % i+ '.jpg'\n",
    "    x = Image.open(filename1)\n",
    "    image_test = x.resize((32,32))\n",
    "    image_test = np.array(image_test)\n",
    "    pic = image_test.reshape((1,1024))\n",
    "    sum1 = np.zeros((32,32))\n",
    "    signal = []\n",
    "    for j in range(1024):\n",
    "        RD_pattern = RD_p[j].reshape((1024,1))\n",
    "        anti_pattern = RD_anti[j].reshape((1024,1))\n",
    "        pattern = RD_p[j].reshape((1024,1)) \n",
    "        temp = np.dot(pic,RD_pattern) - np.dot(pic,anti_pattern)\n",
    "        signal.append(temp)\n",
    "        sum1 = sum1 + temp * pattern.reshape((32,32))\n",
    "    signals.append(signal)\n",
    "#sum1 = normalize(((sum1 - np.min(sum1))/(np.max(sum1) - np.min(sum1)))*256)\n",
    "#image_ghost = Image.fromarray(sum1)\n",
    "#image_ghost = image_ghost.convert('L')\n",
    "sum1[0,0] = np.mean(sum1)\n",
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Sequence Signal\",fontsize =18)\n",
    "plt.xlabel(\"Hadamard Row\")\n",
    "plt.ylabel(\"Idensity\")\n",
    "plt.plot(np.squeeze(signal))\n",
    "\n",
    "plt.subplot(212)\n",
    "signal_sorted = sorted(np.abs(signal),reverse=True)\n",
    "plt.title(\"Sorted Signal\",fontsize =18)\n",
    "plt.plot(np.squeeze(signal_sorted))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"****************** \")\n",
    "print(\"****************** \")\n",
    "print(\"****************** \")\n",
    "\n",
    "print(\"Get the bigger signal ... \")\n",
    "\n",
    "signal_abs = np.abs(np.squeeze(signal))\n",
    "signal_dict = {}\n",
    "j= 0\n",
    "for i in signal_abs:\n",
    "    j = j+1\n",
    "    signal_dict[j] = i \n",
    "\n",
    "xxx =sorted(signal_dict.items(),key = lambda items:items[1],reverse = True)\n",
    "signal_picture = [i[0] for i in xxx]\n",
    "\n",
    "\n",
    "sum2 = np.zeros((32,32))\n",
    "sum3 = np.zeros((32,32))\n",
    "signal = np.squeeze(signal)\n",
    "for j in signal_picture[:NUM_HADAMARD]:\n",
    "    RD_pattern = RD_p[j-1].reshape((1024,1))\n",
    "    anti_pattern = RD_anti[j-1].reshape((1024,1))\n",
    "    pattern = RD_p[j-1].reshape((1024,1)) \n",
    "    temp = np.dot(pic,RD_pattern) - np.dot(pic,anti_pattern)\n",
    "    sum2 = sum2 + temp * pattern.reshape((32,32))\n",
    "y = 0\n",
    "for k in signal[:NUM_HADAMARD]:\n",
    "    y= y+1\n",
    "    pattern = RD_p[y-1].reshape((1024,1)) \n",
    "    sum3 = sum3 + k * pattern.reshape((32,32))\n",
    "#sum1 = normalize(((sum1 - np.min(sum1))/(np.max(sum1) - np.min(sum1)))*256)\n",
    "#image_ghost = Image.fromarray(sum1)\n",
    "#image_ghost = image_ghost.convert('L')\n",
    "sum2[0,0] = np.mean(sum2)\n",
    "sum3[0,0] = np.mean(sum3)\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(\"Full Samples\",fontsize =18)\n",
    "plt.imshow(sum1,cmap = plt.cm.gray)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Bigger \"+str(NUM_HADAMARD),fontsize =18)\n",
    "\n",
    "plt.imshow(sum2,cmap = plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.title(\"First \" +str(NUM_HADAMARD),fontsize =18)\n",
    "plt.imshow(sum3,cmap = plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd \n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (10, 12.0)\n",
    "\n",
    "NUM_HADAMARD = 64\n",
    "\n",
    "def normalize(y):\n",
    "    a,b=np.shape(y)\n",
    "    n = np.zeros((a,b))\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            n[i,j] = np.int(y[i,j])\n",
    "    return n\n",
    "\n",
    "\n",
    "RD_dir = u'C:/Users/zh/Desktop/hadamard/32RD.png'\n",
    "RD=Image.open(RD_dir)\n",
    "RD = np.array(RD)\n",
    "CC = np.ones((1024,1024))\n",
    "RD_p = 2*RD - CC\n",
    "RD_anti = -1 * RD_p\n",
    "#RD_n = -1 * RD_p\n",
    "# K = 256\n",
    "# RD_use =np.vstack((RD_p[0:K,:],RD_n[0:K,:]))  #创造需要使用的 Hadamard 矩阵\n",
    "#NUM_PICTURE = 42000\n",
    "#RATIO = 0.1\n",
    "DIM = 1024 \n",
    "#i = np.random.randint(100)\n",
    "\n",
    "\n",
    "#rand_pattern = np.random.randn(DIM,DIM)\n",
    "signals = []\n",
    "for i in range(40000):\n",
    "    filename1 = 'F:\\Code\\kaggle\\Digit Recognizer\\image\\image' + '%d' % i+ '.jpg'\n",
    "    x = Image.open(filename1)\n",
    "    image_test = x.resize((32,32))\n",
    "    image_test = np.array(image_test)\n",
    "    pic = image_test.reshape((1,1024))\n",
    "    sum1 = np.zeros((32,32))\n",
    "    signal = []\n",
    "    for j in range(1024):\n",
    "        RD_pattern = RD_p[j].reshape((1024,1))\n",
    "        anti_pattern = RD_anti[j].reshape((1024,1))\n",
    "        pattern = RD_p[j].reshape((1024,1)) \n",
    "        temp = np.dot(pic,RD_pattern) - np.dot(pic,anti_pattern)\n",
    "        signal.append(temp)\n",
    "        sum1 = sum1 + temp * pattern.reshape((32,32))\n",
    "    if i%2000 == 0:\n",
    "        print(\"Have finished %d signal\" %i)\n",
    "    signal = np.squeeze(signal)\n",
    "    signals.append(signal)\n",
    "\n",
    "#sig = np.squeeze(signals)\n",
    "data_full_samples = pd.DataFrame(data = signals)\n",
    "data_full_samples.to_csv('F:/Code/signal-signal_fromscratch/data/data_full_samples.csv')\n",
    "\n",
    "print(\"Signals have been written in CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd \n",
    "x = pd.read_csv('F:/Code/signal-signal_fromscratch/data/data_full_samples.csv')\n",
    "original_signals =np.array(x)\n",
    "original_signals =original_signals[:,1:]\n",
    "print(\"We get the original data\")\n",
    "\n",
    "signals_full_sample = np.abs(original_signals)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))*255\n",
    "\n",
    "signals_normalized = []\n",
    "for i in range(signals_full_sample.shape[0]):\n",
    "    one_of_signals = normalize(signals_full_sample[i])\n",
    "    signals_normalized.append(one_of_signals)\n",
    "    \n",
    "\n",
    "signals_normalized = np.array(signals_normalized)\n",
    "print(signals_normalized.shape)\n",
    "\n",
    "print(\"Signals normalize have been finished \")\n",
    "preSort_signals = np.sum(signals_normalized,axis=0)\n",
    "\n",
    "#signal_abs = np.abs(np.squeeze(signal))\n",
    "signal_dict = {}\n",
    "j= 0\n",
    "for i in preSort_signals:\n",
    "    j = j+1\n",
    "    signal_dict[j] = i \n",
    "\n",
    "xxx =sorted(signal_dict.items(),key = lambda items:items[1],reverse = True)\n",
    "collect_sorted_signal= [i[0] for i in xxx]\n",
    "print(\"We get the collective sorted picture index !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (10, 12.0)\n",
    "\n",
    "NUM_HADAMARD = 256\n",
    "\n",
    "def normalize(y):\n",
    "    a,b=np.shape(y)\n",
    "    n = np.zeros((a,b))\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            n[i,j] = np.int(y[i,j])\n",
    "    return n\n",
    "\n",
    "\n",
    "RD_dir = u'C:/Users/zh/Desktop/hadamard/32RD.png'\n",
    "RD=Image.open(RD_dir)\n",
    "RD = np.array(RD)\n",
    "CC = np.ones((1024,1024))\n",
    "RD_p = 2*RD - CC\n",
    "RD_anti = -1 * RD_p\n",
    "sum2 = np.zeros((32,32))\n",
    "for j in collet_sorted_signal[:NUM_HADAMARD]:\n",
    "    RD_pattern = RD_p[j-1].reshape((1024,1))\n",
    "    anti_pattern = RD_anti[j-1].reshape((1024,1))\n",
    "    pattern = RD_p[j-1].reshape((1024,1)) \n",
    "    temp = np.dot(pic,RD_pattern) - np.dot(pic,anti_pattern)\n",
    "    sum2 = sum2 + temp * pattern.reshape((32,32))\n",
    "    \n",
    "plt.title(\"Bigger \"+str(NUM_HADAMARD),fontsize =18)\n",
    "\n",
    "plt.imshow(sum2,cmap = plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = pd.DataFrame(collect_sorted_signal)\n",
    "haha.to_csv('F:/Code/signal-signal_fromscratch/data/collect_sorted_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = [1,3,2,0,6,4,5]\n",
    "xxx =np.array([2,3,4,5,6,7,8])\n",
    "xxx[xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_sorted_signal1 = [x-1 for x in collect_sorted_signal]\n",
    "sorted_signals =original_signals[:,collect_sorted_signal1]\n",
    "sorted_signals_pd = pd.DataFrame(sorted_signals)\n",
    "sorted_signals_pd.to_csv('F:/Code/signal-signal_fromscratch/data/sorted_signals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx =original_signals[3,collect_sorted_signal1]\n",
    "plt.plot(np.abs(testx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def GWN(x,snr):\n",
    "    snr = 10**(snr/10)\n",
    "    xpower = np.sum(x**2)/len(x)\n",
    "    npower = xpower/snr\n",
    "    return x + np.random.randn(len(x))*np.sqrt(npower)\n",
    "\n",
    "#  SNR = 10*log10(xpower/npower) \n",
    "#\n",
    "def cal_snr(x,y,datatype = 'signal'):\n",
    "    if datatype == 'signal':\n",
    "        return 10*np.log10(np.mean(x**2)/np.mean((x-y)**2))\n",
    "    if datatype == 'image':\n",
    "        return 10*np.log10(255**2/np.mean((x-y)**2))\n",
    "   \n",
    "voice = np.linspace(1,5,1000)\n",
    "y = (2)**voice\n",
    "y_noise = GWN(y,10)\n",
    "plt.plot(voice,y,color = 'b')\n",
    "plt.plot(voice,y_noise,color = 'y', linestyle =':')\n",
    "plt.show()\n",
    "\n",
    "print(cal_snr(y,y_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd \n",
    "x = pd.read_csv('F:/Code/signal-signal_fromscratch/data/sorted_signals.csv')\n",
    "original_signals =np.array(x)\n",
    "original_signals =original_signals[:,1:]\n",
    "print(\"We get the original data\")\n",
    "def normalize(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))*255\n",
    "\n",
    "noise_signals = []\n",
    "for i in range(original_signals.shape[0]):\n",
    "    one_of_signals = normalize(original_signals[i,:])\n",
    "    one_of_signals_noise = GWN(one_of_signals,20)\n",
    "    noise_signals.append(one_of_signals_noise)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_snr(normalize(original_signals[6,:]),noise_signals[6][:]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xixi = pd.DataFrame(noise_signals)\n",
    "xixi.to_csv('F:/Code/signal-signal_fromscratch/data/sorted_noise_signals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (10, 12.0)\n",
    "\n",
    "NUM_HADAMARD = 1024\n",
    "\n",
    "def normalize(y):\n",
    "    a,b=np.shape(y)\n",
    "    n = np.zeros((a,b))\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            n[i,j] = np.int(y[i,j])\n",
    "    return n\n",
    "sort_index = pd.read_csv('F:/Code/signal-signal_fromscratch/data/collect_sorted_index.csv')\n",
    "sort_index = np.array(sort_index)\n",
    "sort_index = sort_index[:,1:]\n",
    "sort_index = [x-1 for x in sort_index]\n",
    "RD_dir = u'C:/Users/zh/Desktop/hadamard/32RD.png'\n",
    "RD=Image.open(RD_dir)\n",
    "RD = np.array(RD)\n",
    "CC = np.ones((1024,1024))\n",
    "RD_p = 2*RD - CC\n",
    "RD_anti = -1 * RD_p\n",
    "sum3 = np.zeros((32,32))\n",
    "y = 0\n",
    "noise_signals_1 = noise_signals[1][:]\n",
    "for k in noise_signals_1[:NUM_HADAMARD]:\n",
    "    y= y+1\n",
    "    pattern = RD_p[sort_index[y-1]].reshape((1024,1)) \n",
    "    sum3 = sum3 + k * pattern.reshape((32,32))\n",
    "sum3[0,0] = np.mean(sum3)  \n",
    "plt.title(\"First \" +str(NUM_HADAMARD),fontsize =18)\n",
    "plt.imshow(sum3,cmap = plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "Please wait for a while ...\n",
      "epoch 0 loss 54932928.0000 \n",
      "Evaluating ...\n",
      "Saving model....\n",
      "epoch 2 loss 51637160.0000 \n",
      "epoch 4 loss 47069548.0000 \n",
      "epoch 6 loss 43355916.0000 \n",
      "epoch 8 loss 40146968.0000 \n",
      "epoch 10 loss 37158736.0000 \n",
      "Evaluating ...\n",
      "Saving model....\n",
      "epoch 12 loss 34604984.0000 \n",
      "epoch 14 loss 32583748.0000 \n",
      "epoch 16 loss 30936788.0000 \n",
      "epoch 18 loss 29510492.0000 \n",
      "epoch 20 loss 28245358.0000 \n",
      "Evaluating ...\n",
      "Saving model....\n",
      "epoch 22 loss 27114424.0000 \n",
      "epoch 24 loss 26105124.0000 \n",
      "epoch 26 loss 25201544.0000 \n",
      "epoch 28 loss 24386972.0000 \n",
      "epoch 30 loss 23660536.0000 \n",
      "Evaluating ...\n",
      "Saving model....\n",
      "epoch 32 loss 23002950.0000 \n",
      "epoch 34 loss 22398088.0000 \n",
      "epoch 36 loss 21846692.0000 \n",
      "epoch 38 loss 21335368.0000 \n",
      "epoch 40 loss 20858246.0000 \n",
      "Evaluating ...\n",
      "Saving model....\n",
      "epoch 42 loss 20408012.0000 \n",
      "epoch 44 loss 19988854.0000 \n",
      "epoch 46 loss 19591632.0000 \n",
      "epoch 48 loss 19219078.0000 \n",
      "The train has been done . -_-\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "x = load_data('F:/Code/signal-signal_fromscratch/data/sorted_noise_signals.csv')\n",
    "y = load_data('F:/Code/signal-signal_fromscratch/data/sorted_signals.csv')\n",
    "\n",
    "x_train_data = x[:39895,:256]\n",
    "y_train_data = y[:39895,:256]\n",
    "x_test_data = x[39895:39995,:256]\n",
    "y_test_data = y[39895:39995,:256]\n",
    "x_eval_data = x[39995:,:256]\n",
    "y_eval_data = y[39995:,:256]\n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "train(x_train_data,y_train_data,x_eval_data,y_eval_data,sort_index,RD_p,epoch = 50)\n",
    "\n",
    "print(\"The train has been done . -_-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(3,2)\n",
    "print(z)\n",
    "np.array([z[1,:]]).shape\n",
    "np.array(np.squeeze([z[1,:]])).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
